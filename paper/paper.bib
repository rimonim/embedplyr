
@misc{atari2023,
	title = {Contextualized {Construct} {Representation}: {Leveraging} {Psychometric} {Scales} to {Advance} {Theory}-{Driven} {Text} {Analysis}},
	url = {osf.io/preprints/psyarxiv/m93pd},
	publisher = {PsyArXiv},
	author = {Atari, Mohammad and Omrani, Ali and Dehghani, Morteza},
	month = feb,
	year = {2023},
	doi = {10.31234/osf.io/m93pd},
}

@article{duran2019,
	title = {{ALIGN}: {Analyzing} {Linguistic} {Interactions} {With} {Generalizable} {techNiques}-{A} {Python} {Library}},
	volume = {24},
	copyright = {2019 American Psychological Association},
	issn = {1082-989X},
	doi = {10.31234/osf.io/a5yh9},
	abstract = {Linguistic alignment (LA) is the tendency during a conversation to reuse each other's linguistic expressions, including lexical, conceptual, or syntactic structures. LA is often argued to be a crucial driver in reciprocal understanding and interpersonal rapport, though its precise dynamics and effects are still controversial. One barrier to more systematic investigation of these effects lies in the diversity in the methods employed to analyze LA, which makes it difficult to integrate and compare results of individual studies. To overcome this issue, we have developed ALIGN (Analyzing Linguistic Interactions with Generalizable techNiques), an open-source Python package to measure LA in conversation (https://pypi.python.org/pypi/align) along with in-depth open-source tutorials hosted on ALIGN's GitHub repository (https://github.com/nickduran/align-linguistic-alignment). Here, we first describe the challenges in the study of LA and outline how ALIGN can address them. We then demonstrate how our analytical protocol can be applied to theory-driven questions using a complex corpus of dialogue (the Devil's Advocate corpus; Duran \& Fusaroli, 2017). We close by identifying further challenges and point to future developments of the field.},
	number = {4},
	journal = {Psychological methods},
	author = {Duran, Nicholas D. and Paxton, Alexandra and Fusaroli, Riccardo},
	year = {2019},
	note = {Place: United States
Publisher: American Psychological Association},
	pages = {419--438},
}

@misc{kjell2023,
	title = {The text-package: {An} {R}-package for {Analyzing} and {Visualizing} {Human} {Language} {Using} {Natural} {Language} {Processing} and {Deep} {Learning}},
	url = {osf.io/preprints/psyarxiv/293kt},
	publisher = {PsyArXiv},
	author = {Kjell, Oscar N E and Giorgi, Salvatore and Schwartz, H. A},
	year = {2023},
	doi = {10.31234/osf.io/293kt},
}

@article{grand2022,
	title = {Semantic projection recovers rich human knowledge of multiple object features from word embeddings},
	volume = {6},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-022-01316-8},
	doi = {10.1038/s41562-022-01316-8},
	abstract = {How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein words that are used in more similar linguistic contexts—that is, are more semantically related—are located closer together. However, whereas inter-word proximity captures only overall relatedness, human judgements are highly context dependent. For example, dolphins and alligators are similar in size but differ in dangerousness. Here, we use a domain-general method to extract context-dependent relationships from word embeddings: ‘semantic projection’ of word-vectors onto lines that represent features such as size (the line connecting the words ‘small’ and ‘big’) or danger (‘safe’ to ‘dangerous’), analogous to ‘mental scales’. This method recovers human judgements across various object categories and properties. Thus, the geometry of word embeddings explicitly represents a wealth of context-dependent world knowledge.},
	language = {en},
	number = {7},
	urldate = {2024-04-07},
	journal = {Nature Human Behaviour},
	author = {Grand, Gabriel and Blank, Idan Asher and Pereira, Francisco and Fedorenko, Evelina},
	month = jul,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Psychology, Human behaviour, Language and linguistics},
	pages = {975--987},
	file = {Accepted Version:/Users/louisteitelbaum/Zotero/storage/U4WT6QLJ/Grand et al. - 2022 - Semantic projection recovers rich human knowledge .pdf:application/pdf},
}

@article{deerwester1990,
	title = {Indexing by latent semantic analysis},
	volume = {41},
	copyright = {Copyright © 1990 John Wiley \& Sons, Inc.},
	issn = {1097-4571},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
	doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
	abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
	language = {en},
	number = {6},
	urldate = {2024-05-01},
	journal = {Journal of the American Society for Information Science},
	author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	year = {1990},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-4571\%28199009\%2941\%3A6\%3C391\%3A\%3AAID-ASI1\%3E3.0.CO\%3B2-9},
	pages = {391--407},
	file = {Submitted Version:/Users/louisteitelbaum/Zotero/storage/URVVFR5I/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:application/pdf},
}

@misc{mikolov2013a,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/louisteitelbaum/Zotero/storage/H6AV6ZCW/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/Users/louisteitelbaum/Zotero/storage/M77SDR2Q/1301.html:text/html},
}

@misc{mikolov2013b,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	doi = {10.48550/arXiv.1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv:1310.4546 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/louisteitelbaum/Zotero/storage/3DYDHGNJ/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf;arXiv.org Snapshot:/Users/louisteitelbaum/Zotero/storage/S8JEVMPG/1310.html:text/html},
}

@misc{bojanowski2017a,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	url = {http://arxiv.org/abs/1607.04606},
	doi = {10.48550/arXiv.1607.04606},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = jun,
	year = {2017},
	note = {arXiv:1607.04606 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to TACL. The two first authors contributed equally},
	file = {arXiv Fulltext PDF:/Users/louisteitelbaum/Zotero/storage/W5S9SLHF/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:application/pdf;arXiv.org Snapshot:/Users/louisteitelbaum/Zotero/storage/RADH424I/1607.html:text/html},
}

@misc{grave2018,
	title = {Learning {Word} {Vectors} for 157 {Languages}},
	url = {http://arxiv.org/abs/1802.06893},
	doi = {10.48550/arXiv.1802.06893},
	abstract = {Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
	month = mar,
	year = {2018},
	note = {arXiv:1802.06893 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to LREC},
	file = {arXiv Fulltext PDF:/Users/louisteitelbaum/Zotero/storage/3LQNDG9U/Grave et al. - 2018 - Learning Word Vectors for 157 Languages.pdf:application/pdf;arXiv.org Snapshot:/Users/louisteitelbaum/Zotero/storage/3GED86HP/1802.html:text/html},
}

@inproceedings{choi2024,
	address = {Cham},
	title = {Word {Embedding}-{Based} {Text} {Complexity} {Analysis}},
	isbn = {978-3-031-57867-0},
	doi = {10.1007/978-3-031-57867-0_21},
	abstract = {Text complexity metrics serve crucial roles in quantifying the readability level of important documents, leading to ensuring public safety, enhancing educational outcomes, and more. Pointwise mutual information (PMI) has been widely used to measure text complexity by capturing the statistical co-occurrence patterns between word pairs, assuming their semantic significance. However, we observed that word embeddings are similar to PMI in that both are based on co-occurrence in large corpora. Yet, word embeddings are superior in terms of faster calculations and more generalizable semantic proximity measures. Given this, we propose a novel text complexity metric that leverages the power of word embeddings to measure the semantic distance between words in a document. We empirically validate our approach by analyzing the OneStopEnglish dataset, which contains news articles annotated with expert-labeled readability scores. Our experiments reveal that the proposed word embedding-based metric demonstrates a stronger correlation with ground-truth readability levels than conventional PMI-based metrics. This study serves as a cornerstone for future research aiming to incorporate context-dependent embeddings and extends applicability to various text types.},
	language = {en},
	booktitle = {Wisdom, {Well}-{Being}, {Win}-{Win}},
	publisher = {Springer Nature Switzerland},
	author = {Choi, Kahyun},
	editor = {Sserwanga, Isaac and Joho, Hideo and Ma, Jie and Hansen, Preben and Wu, Dan and Koizumi, Masanori and Gilliland, Anne J.},
	year = {2024},
	keywords = {Pointwise Mutual Information, Readability, Text complexity, Word embedding},
	pages = {283--292},
	file = {Full Text PDF:/Users/louisteitelbaum/Zotero/storage/SIJ7WMZR/Choi - 2024 - Word Embedding-Based Text Complexity Analysis.pdf:application/pdf},
}

@article{schonemann1966,
	title = {A generalized solution of the orthogonal procrustes problem},
	volume = {31},
	issn = {1860-0980},
	url = {10.1007/BF02289451},
	doi = {10.1007/BF02289451},
	abstract = {A solutionT of the least-squares problemAT=B +E, givenA andB so that trace (E′E)= minimum andT′T=I is presented. It is compared with a less general solution of the same problem which was given by Green [5]. The present solution, in contrast to Green's, is applicable to matricesA andB which are of less than full column rank. Some technical suggestions for the numerical computation ofT and an illustrative example are given.},
	language = {en},
	number = {1},
	urldate = {2025-01-02},
	journal = {Psychometrika},
	author = {Schönemann, Peter H.},
	month = mar,
	year = {1966},
	keywords = {General Solution, Numerical Computation, Present Solution, Public Policy, Statistical Theory},
	pages = {1--10},
}

@book{bao2023,
	title = {{PsychWordVec}: {Word} {Embedding} {Research} {Framework} for {Psychological} {Science}},
	url = {https://psychbruce.github.io/PsychWordVec/},
	author = {Bao, Han-Wu-Shuang},
	year = {2023},
	doi = {10.32614/cran.package.psychwordvec},
	annote = {R package version 2023.9},
}

@article{benoit2018,
	title = {quanteda: {An} {R} package for the quantitative analysis of textual data},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {quanteda},
	url = {https://joss.theoj.org/papers/10.21105/joss.00774},
	doi = {10.21105/joss.00774},
	abstract = {Benoit et al., (2018). quanteda: An R package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774, https://doi.org/10.21105/joss.00774},
	language = {en},
	number = {30},
	urldate = {2025-01-03},
	journal = {Journal of Open Source Software},
	author = {Benoit, Kenneth and Watanabe, Kohei and Wang, Haiyan and Nulty, Paul and Obeng, Adam and Müller, Stefan and Matsuo, Akitaka},
	month = oct,
	year = {2018},
	pages = {774},
	file = {Full Text PDF:/Users/louisteitelbaum/Zotero/storage/3AMP72V4/Benoit et al. - 2018 - quanteda An R package for the quantitative analys.pdf:application/pdf},
}

@article{garten2018,
	title = {Dictionaries and distributions: {Combining} expert knowledge and large scale textual data content analysis : {Distributed} dictionary representation},
	volume = {50},
	issn = {1554-3528},
	shorttitle = {Dictionaries and distributions},
	doi = {10.3758/s13428-017-0875-9},
	abstract = {Theory-driven text analysis has made extensive use of psychological concept dictionaries, leading to a wide range of important results. These dictionaries have generally been applied through word count methods which have proven to be both simple and effective. In this paper, we introduce Distributed Dictionary Representations (DDR), a method that applies psychological dictionaries using semantic similarity rather than word counts. This allows for the measurement of the similarity between dictionaries and spans of text ranging from complete documents to individual words. We show how DDR enables dictionary authors to place greater emphasis on construct validity without sacrificing linguistic coverage. We further demonstrate the benefits of DDR on two real-world tasks and finally conduct an extensive study of the interaction between dictionary size and task performance. These studies allow us to examine how DDR and word count methods complement one another as tools for applying concept dictionaries and where each is best applied. Finally, we provide references to tools and resources to make this method both available and accessible to a broad psychological audience.},
	language = {eng},
	number = {1},
	journal = {Behavior Research Methods},
	author = {Garten, Justin and Hoover, Joe and Johnson, Kate M. and Boghrati, Reihane and Iskiwitch, Carol and Dehghani, Morteza},
	month = feb,
	year = {2018},
	pmid = {28364281},
	keywords = {Data Mining, Dictionary-based text analysis, Humans, Linguistics, Methodological innovation, Psychology, Semantic representation, Semantics, Task Performance and Analysis, Text analysis, Vocabulary},
	pages = {344--361},
	file = {Full Text:/Users/louisteitelbaum/Zotero/storage/GW7PM6T3/Garten et al. - 2018 - Dictionaries and distributions Combining expert k.pdf:application/pdf},
}

@misc{ethayarajh2019,
	title = {Towards {Understanding} {Linear} {Word} {Analogies}},
	url = {http://arxiv.org/abs/1810.04882},
	doi = {10.48550/arXiv.1810.04882},
	abstract = {A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
	month = aug,
	year = {2019},
	note = {arXiv:1810.04882 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to ACL 2019},
	file = {Preprint PDF:/Users/louisteitelbaum/Zotero/storage/AC9PHLUN/Ethayarajh et al. - 2019 - Towards Understanding Linear Word Analogies.pdf:application/pdf;Snapshot:/Users/louisteitelbaum/Zotero/storage/9TYRHV6Q/1810.html:text/html},
}

@article{schrimpf2021,
	title = {The neural architecture of language: {Integrative} modeling converges on predictive processing},
	volume = {118},
	issn = {0027-8424, 1091-6490},
	shorttitle = {The neural architecture of language},
	url = {https://pnas.org/doi/full/10.1073/pnas.2105646118},
	doi = {10.1073/pnas.2105646118},
	abstract = {Significance
            Language is a quintessentially human ability. Research has long probed the functional architecture of language in the mind and brain using diverse neuroimaging, behavioral, and computational modeling approaches. However, adequate neurally-mechanistic accounts of how meaning might be extracted from language are sorely lacking. Here, we report a first step toward addressing this gap by connecting recent artificial neural networks from machine learning to human recordings during language processing. We find that the most powerful models predict neural and behavioral responses across different datasets up to noise levels. Models that perform better at predicting the next word in a sequence also better predict brain measurements—providing computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the brain.
          ,
            The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species’ signature cognitive skill. We find that the most powerful “transformer” models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models’ neural fits (“brain score”) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
	language = {en},
	number = {45},
	urldate = {2025-01-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = nov,
	year = {2021},
	pages = {e2105646118},
	file = {Full Text:/Users/louisteitelbaum/Zotero/storage/R5STKDGK/Schrimpf et al. - 2021 - The neural architecture of language Integrative m.pdf:application/pdf},
}

@misc{hamilton2018,
	title = {Diachronic {Word} {Embeddings} {Reveal} {Statistical} {Laws} of {Semantic} {Change}},
	url = {http://arxiv.org/abs/1605.09096},
	doi = {10.48550/arXiv.1605.09096},
	abstract = {Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
	month = oct,
	year = {2018},
	note = {arXiv:1605.09096 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Association for Computational Linguistics (ACL), 2016. Minor corrections; improved methodology for Section 3},
	file = {Full Text PDF:/Users/louisteitelbaum/Zotero/storage/NUBRV7B7/Hamilton et al. - 2018 - Diachronic Word Embeddings Reveal Statistical Laws.pdf:application/pdf;Snapshot:/Users/louisteitelbaum/Zotero/storage/EGGSMXKW/1605.html:text/html},
}

@article{kozlowski2019,
	title = {The {Geometry} of {Culture}: {Analyzing} the {Meanings} of {Class} through {Word} {Embeddings}},
	volume = {84},
	issn = {0003-1224},
	shorttitle = {The {Geometry} of {Culture}},
	url = {https://doi.org/10.1177/0003122419877135},
	doi = {10.1177/0003122419877135},
	abstract = {We argue word embedding models are a useful tool for the study of culture using a historical analysis of shared understandings of social class as an empirical case. Word embeddings represent semantic relations between words as relationships between vectors in a high-dimensional space, specifying a relational model of meaning consistent with contemporary theories of culture. Dimensions induced by word differences (rich – poor) in these spaces correspond to dimensions of cultural meaning, and the projection of words onto these dimensions reflects widely shared associations, which we validate with surveys. Analyzing text from millions of books published over 100 years, we show that the markers of class continuously shifted amidst the economic transformations of the twentieth century, yet the basic cultural dimensions of class remained remarkably stable. The notable exception is education, which became tightly linked to affluence independent of its association with cultivated taste.},
	language = {en},
	number = {5},
	urldate = {2025-01-05},
	journal = {American Sociological Review},
	author = {Kozlowski, Austin C. and Taddy, Matt and Evans, James A.},
	month = oct,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	pages = {905--949},
	file = {Submitted Version:/Users/louisteitelbaum/Zotero/storage/IUWT7L2W/Kozlowski et al. - 2019 - The Geometry of Culture Analyzing the Meanings of.pdf:application/pdf},
}

@book{teitelbaum2024,
	title = {Data {Science} for {Psychology}: {Natural} {Language}},
	shorttitle = {Data {Science} for {Psychology}},
	url = {https://zenodo.org/records/10908367},
	abstract = {This book covers state-of-the-art methods for analyzing psychological properties of text, including the fundamentals of data visualization, data collection, and scientific methodology necessary to produce meaningful work in the field.

At every step of the way, we give examples in R, using the tools and rules of the tidyverse. This book also covers the basics of the quanteda and text packages for natural language processing (NLP), and the vosonSML package for collecting data from popular social media sites.

The book is published at ds4psych.com},
	urldate = {2025-01-05},
	publisher = {Computational Social Psychology Lab},
	author = {Teitelbaum, Louis and Simchon, Almog},
	month = apr,
	year = {2024},
	doi = {10.5281/zenodo.10908367},
	note = {Version Number: v0.1.0},
	keywords = {Data science, data visualization, Natural language processing, Psychology, R, sentiment analysis, Social psychology},
}

@article{wickham2019,
	title = {Welcome to the {Tidyverse}},
	volume = {4},
	issn = {2475-9066},
	url = {https://joss.theoj.org/papers/10.21105/joss.01686},
	doi = {10.21105/joss.01686},
	abstract = {Wickham et al., (2019). Welcome to the Tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686},
	language = {en},
	number = {43},
	urldate = {2025-01-05},
	journal = {Journal of Open Source Software},
	author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and François, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and Müller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
	month = nov,
	year = {2019},
	pages = {1686},
	file = {Full Text PDF:/Users/louisteitelbaum/Zotero/storage/J935AK82/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf:application/pdf},
}

@article{feuerriegel2025,
	title = {Using natural language processing to analyse text data in behavioural science},
	copyright = {2025 Springer Nature America, Inc.},
	issn = {2731-0574},
	url = {https://www.nature.com/articles/s44159-024-00392-z},
	doi = {10.1038/s44159-024-00392-z},
	abstract = {Language is a uniquely human trait at the core of human interactions. The language people use often reflects their personality, intentions and state of mind. With the integration of the Internet and social media into everyday life, much of human communication is documented as written text. These online forms of communication (for example, blogs, reviews, social media posts and emails) provide a window into human behaviour and therefore present abundant research opportunities for behavioural science. In this Review, we describe how natural language processing (NLP) can be used to analyse text data in behavioural science. First, we review applications of text data in behavioural science. Second, we describe the NLP pipeline and explain the underlying modelling approaches (for example, dictionary-based approaches and large language models). We discuss the advantages and disadvantages of these methods for behavioural science, in particular with respect to the trade-off between interpretability and accuracy. Finally, we provide actionable recommendations for using NLP to ensure rigour and reproducibility. Natural language processing (NLP) methods are growing in popularity as they become cheaper to implement and easier to use. In this Review, Feuerriegel et al. describe NLP methods and provide recommendations for the use of NLP in behavioural science.},
	language = {en},
	urldate = {2025-01-05},
	journal = {Nature Reviews Psychology},
	author = {Feuerriegel, Stefan and Maarouf, Abdurahman and Bär, Dominik and Geissler, Dominique and Schweisthal, Jonas and Pröllochs, Nicolas and Robertson, Claire E. and Rathje, Steve and Hartmann, Jochen and Mohammad, Saif M. and Netzer, Oded and Siegel, Alexandra A. and Plank, Barbara and Van Bavel, Jay J.},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Language and linguistics, Psychology},
	pages = {1--16},
}

@article{lauriola2022,
	title = {An introduction to {Deep} {Learning} in {Natural} {Language} {Processing}: {Models}, techniques, and tools},
	volume = {470},
	issn = {0925-2312},
	shorttitle = {An introduction to {Deep} {Learning} in {Natural} {Language} {Processing}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221010997},
	doi = {10.1016/j.neucom.2021.05.103},
	abstract = {Natural Language Processing (NLP) is a branch of artificial intelligence that involves the design and implementation of systems and algorithms able to interact through human language. Thanks to the recent advances of deep learning, NLP applications have received an unprecedented boost in performance. In this paper, we present a survey of the application of deep learning techniques in NLP, with a focus on the various tasks where deep learning is demonstrating stronger impact. Additionally, we explore, describe, and revise the main resources in NLP research, including software, hardware, and popular corpora. Finally, we emphasize the main limits of deep learning in NLP and current research directions.},
	urldate = {2025-01-05},
	journal = {Neurocomputing},
	author = {Lauriola, Ivano and Lavelli, Alberto and Aiolli, Fabio},
	month = jan,
	year = {2022},
	keywords = {Deep Learning, Language Models, Natural Language Processing, Software, Transformer},
	pages = {443--456},
	file = {ScienceDirect Snapshot:/Users/louisteitelbaum/Zotero/storage/PL7CBMCL/S0925231221010997.html:text/html},
}

@misc{wijffels2023,
	title = {word2vec: {Distributed} {Representations} of {Words}},
	copyright = {Apache License (≥ 2.0)},
	shorttitle = {word2vec},
	url = {https://cran.r-project.org/web/packages/word2vec/index.html},
	doi = {10.32614/cran.package.word2vec},
	abstract = {Learn vector representations of words by continuous bag of words and skip-gram implementations of the 'word2vec' algorithm. The techniques are detailed in the paper "Distributed Representations of Words and Phrases and their Compositionality" by Mikolov et al. (2013), available at {\textless}doi:10.48550/arXiv.1310.4546{\textgreater}.},
	urldate = {2025-01-05},
	author = {Wijffels, Jan and Watanabe, Kohei and Fomichev, Max},
	month = oct,
	year = {2023},
	keywords = {NaturalLanguageProcessing},
}

@misc{selivanov2023,
	title = {text2vec: {Modern} {Text} {Mining} {Framework} for {R}},
	copyright = {GPL-2 {\textbar} GPL-3 {\textbar} file LICENSE [expanded from: GPL (≥ 2) {\textbar} file LICENSE]},
	shorttitle = {text2vec},
	url = {https://cran.r-project.org/web/packages/text2vec/index.html},
	doi = {10.32614/cran.package.text2vec},
	abstract = {Fast and memory-friendly tools for text vectorization, topic modeling (LDA, LSA), word embeddings (GloVe), similarities. This package provides a source-agnostic streaming API, which allows researchers to perform analysis of collections of documents which are larger than available RAM. All core functions are parallelized to benefit from multicore machines.},
	urldate = {2025-01-05},
	author = {Selivanov, Dmitriy and Bickel, Manuel and Wang, Qing},
	month = nov,
	year = {2023},
	keywords = {NaturalLanguageProcessing},
}

@article{blei2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	issn = {1532-4435},
	doi = {https://doi.org/10.7551/mitpress/1120.003.0082},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	month = mar,
	year = {2003},
	pages = {993--1022},
	file = {Full Text PDF:/Users/louisteitelbaum/Zotero/storage/XADFFVTP/Blei et al. - 2003 - Latent dirichlet allocation.pdf:application/pdf},
}

@inproceedings{pennington2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2025-01-05},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:/Users/louisteitelbaum/Zotero/storage/7CX7JWZX/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
}

@inproceedings{wolf2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://aclanthology.org/2020.emnlp-demos.6/},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2025-01-05},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	editor = {Liu, Qun and Schlangen, David},
	month = oct,
	year = {2020},
	pages = {38--45},
	file = {Full Text PDF:/Users/louisteitelbaum/Zotero/storage/CC5LBWVK/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf:application/pdf},
}

@misc{carrella2023,
	title = {The '{Truth} {Contagion}' {Effect} in the {US} {Political} {Online} {Debate}},
	url = {https://osf.io/qx34w},
	doi = {10.31234/osf.io/qx34w},
	abstract = {During the last decade, evidence shows that US politicians’ conception of honesty has undergone a distinct bifurcation, with authentic and sincere but evidence-free "belief-speaking'' becoming more prominent and more differentiated from evidence-based "fact speaking''. Here we examine the downstream consequences of those two ways of conceiving honesty by investigating how users engage with fact-speaking and belief-speaking texts from Democratic and Republican politicians on Twitter. We measured the conceptions of honesty of a sample of tweets and replies using computational text processing, and checked whether the conceptions of honesty in the original tweets aligned with those in their replies. We also measured polarizing language in the replies and examined how it related to the honesty components in the tweets. We found that the conceptions of honesty used in replies aligned with those of the original tweets, suggesting a "contagion'' effect such that politicians determine the tenor of a subsequent conversation involving the public. We also found that belief-speaking tweets were more likely to trigger polarized language in replies, whereas fact-speaking tweets had the opposite effect. Our analysis highlights the crucial role of political leaders in setting the tone of the conversation on social media and it illustrates how evidence-based communication could help decrease audience polarization in online political debate.},
	language = {en-us},
	urldate = {2025-01-05},
	publisher = {OSF},
	author = {Carrella, Fabio and Aroyehun, Segun Taofeek and Lasser, Jana and Simchon, Almog and Garcia, David and Lewandowsky, Stephan},
	month = dec,
	year = {2023},
	keywords = {affective polarization, honesty, political discourse, social media},
	file = {OSF Preprint:/Users/louisteitelbaum/Zotero/storage/I8HUGDWA/Carrella et al. - 2023 - The 'Truth Contagion' Effect in the US Political O.pdf:application/pdf},
}

@misc{hussain2024,
	title = {Probing the contents of semantic representations from text, behavior, and brain data using the {psychNorms} metabase},
	url = {http://arxiv.org/abs/2412.04936},
	doi = {10.48550/arXiv.2412.04936},
	abstract = {Semantic representations are integral to natural language processing, psycholinguistics, and artificial intelligence. Although often derived from internet text, recent years have seen a rise in the popularity of behavior-based (e.g., free associations) and brain-based (e.g., fMRI) representations, which promise improvements in our ability to measure and model human representations. We carry out the first systematic evaluation of the similarities and differences between semantic representations derived from text, behavior, and brain data. Using representational similarity analysis, we show that word vectors derived from behavior and brain data encode information that differs from their text-derived cousins. Furthermore, drawing on our psychNorms metabase, alongside an interpretability method that we call representational content analysis, we find that, in particular, behavior representations capture unique variance on certain affective, agentic, and socio-moral dimensions. We thus establish behavior as an important complement to text for capturing human representations and behavior. These results are broadly relevant to research aimed at learning human-aligned semantic representations, including work on evaluating and aligning large language models.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Hussain, Zak and Mata, Rui and Newell, Ben R. and Wulff, Dirk U.},
	month = dec,
	year = {2024},
	note = {arXiv:2412.04936 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 13 pages, 5 figures, 2 tables},
	file = {Preprint PDF:/Users/louisteitelbaum/Zotero/storage/MMFU7VY7/Hussain et al. - 2024 - Probing the contents of semantic representations f.pdf:application/pdf;Snapshot:/Users/louisteitelbaum/Zotero/storage/VTMBX896/2412.html:text/html},
}

@misc{markus2024,
	title = {Reap the {Wild} {Wind}: {Detecting} {Media} {Storms} in {Large}-{Scale} {News} {Corpora}},
	shorttitle = {Reap the {Wild} {Wind}},
	url = {http://arxiv.org/abs/2404.09299},
	doi = {10.48550/arXiv.2404.09299},
	abstract = {Media Storms, dramatic outbursts of attention to a story, are central components of media dynamics and the attention landscape. Despite their significance, there has been little systematic and empirical research on this concept due to issues of measurement and operationalization. We introduce an iterative human-in-the-loop method to identify media storms in a large-scale corpus of news articles. The text is first transformed into signals of dispersion based on several textual characteristics. In each iteration, we apply unsupervised anomaly detection to these signals; each anomaly is then validated by an expert to confirm the presence of a storm, and those results are then used to tune the anomaly detection in the next iteration. We demonstrate the applicability of this method in two scenarios: first, supplementing an initial list of media storms within a specific time frame; and second, detecting media storms in new time periods. We make available a media storm dataset compiled using both scenarios. Both the method and dataset offer the basis for comprehensive empirical research into the concept of media storms, including characterizing them and predicting their outbursts and durations, in mainstream media or social media platforms.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Markus, Dror K. and Levi, Effi and Sheafer, Tamir and Shenhav, Shaul R.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09299 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/louisteitelbaum/Zotero/storage/39SDGAIR/Markus et al. - 2024 - Reap the Wild Wind Detecting Media Storms in Larg.pdf:application/pdf;Snapshot:/Users/louisteitelbaum/Zotero/storage/WF8IHCRF/2404.html:text/html},
}

@book{wickham2023,
	title = {dplyr: {A} {Grammar} of {Data} {Manipulation}},
	url = {https://dplyr.tidyverse.org},
	author = {Wickham, Hadley and François, Romain and Henry, Lionel and Müller, Kirill and Vaughan, Davis},
	year = {2023},
	annote = {R package version 1.1.4, https://github.com/tidyverse/dplyr},
}

@book{mouselimis2024,
	title = {{fastText}: {Efficient} {Learning} of {Word} {Representations} and {Sentence} {Classification} using {R}},
	url = {https://CRAN.R-project.org/package=fastText},
	author = {Mouselimis, Lampros},
	year = {2024},
	annote = {R package version 1.0.4},
}

@book{facebook2016,
	title = {{fastText}: {Library} for fast text representation and classification},
	url = {https://github.com/facebookresearch/fastText},
	author = {Facebook, Inc},
	year = {2016},
}

@inproceedings{bojanowski2017b,
	title = {Bag of {Tricks} for {Efficient} {Text} {Classification}},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Volume} 2, {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	year = {2017},
	doi = {10.48550/arXiv.1607.01759},
	pages = {427--431},
}

@inproceedings{meng2019,
	title = {Spherical {Text} {Embedding}},
	url = {https://github.com/yumeng5/Spherical-Text-Embedding},
	booktitle = {Advances in neural information processing systems},
	author = {Meng, Yu and Huang, Jiaxin and Wang, Guangyuan and Zhang, Chao and Zhuang, Honglei and Kaplan, Lance and Han, Jiawei},
	year = {2019},
}

@article{boyd2022,
	title = {The development and psychometric properties of {LIWC}-22},
	volume = {10},
	journal = {Austin, TX: University of Texas at Austin},
	author = {Boyd, Ryan L and Ashokkumar, Ashwini and Seraj, Sarah and Pennebaker, James W},
	year = {2022},
}

@book{cardot2022,
	title = {Gmedian: {Geometric} {Median}, k-{Medians} {Clustering} and {Robust} {Median} {PCA}},
	url = {https://CRAN.R-project.org/package=Gmedian},
	author = {Cardot, Herve},
	year = {2022},
	doi = {10.32614/cran.package.gmedian},
	annote = {R package version 1.2.7},
}

@inproceedings{arora2017,
	title = {A simple but tough-to-beat baseline for sentence embeddings},
	booktitle = {International conference on learning representations},
	author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
	year = {2017},
}
