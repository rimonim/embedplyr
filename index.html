<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Tools for Working With Text Embeddings • embedplyr</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="apple-touch-icon-60x60.png">
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Tools for Working With Text Embeddings">
<meta name="description" content='Common operations with word and text embeddings within a tidyverse/quanteda workflow, as demonstrated in "Data Science for Psychology: Natural Language". Includes simple functions for calculating common similarity metrics, as well as higher level functions for loading pretrained word embedding models (e.g. GloVe), applying them to words, aggregating to produce text embeddings, and reducing dimensionality.'>
<meta property="og:description" content='Common operations with word and text embeddings within a tidyverse/quanteda workflow, as demonstrated in "Data Science for Psychology: Natural Language". Includes simple functions for calculating common similarity metrics, as well as higher level functions for loading pretrained word embedding models (e.g. GloVe), applying them to words, aggregating to produce text embeddings, and reducing dimensionality.'>
<meta property="og:image" content="https://rimonim.github.io/embedplyr/logo.png">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">embedplyr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="articles/DDR.html">Distributed Dictionary Representation (DDR)</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header">
<img src="logo.png" class="logo" alt=""><h1 id="embedplyr-">embedplyr <a class="anchor" aria-label="anchor" href="#embedplyr-"></a>
</h1>
</div>
<!-- badges: start -->

<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p>embedplyr enables common operations with word and text embeddings within a ‘tidyverse’ and/or ‘quanteda’ workflow, as demonstrated in <a href="http://ds4psych.com" class="external-link">Data Science for Psychology: Natural Language</a>.</p>
<ul>
<li>
<code><a href="reference/load_embeddings.html">load_embeddings()</a></code> loads pretrained <a href="https://nlp.stanford.edu/projects/glove/" class="external-link">GloVe</a>, <a href="https://code.google.com/archive/p/word2vec/" class="external-link">word2vec</a>, <a href="https://github.com/commonsense/conceptnet-numberbatch" class="external-link">ConceptNet Numberbatch</a>, and <a href="https://fasttext.cc" class="external-link">fastText</a> word embedding models from Internet sources or from your working directory</li>
<li>
<code><a href="reference/embed_tokens.html">embed_tokens()</a></code> returns the embedding for each token in a set of texts</li>
<li>
<code><a href="reference/textstat_embedding.html">embed_docs()</a></code> generates text embeddings for a set of documents</li>
<li>
<code><a href="reference/get_sims.html">get_sims()</a></code> calculates row-wise similarity metrics between a set of embeddings and a given reference</li>
<li>
<code><a href="reference/average_embedding.html">average_embedding()</a></code> calculates the (weighted) average of multiple embeddings</li>
<li>
<code><a href="reference/reduce_dimensionality.html">reduce_dimensionality()</a></code> reduces the dimensionality of embeddings</li>
<li>
<code><a href="reference/normalize.html">normalize()</a></code> and <code><a href="reference/normalize.html">normalize_rows()</a></code> normalize embeddings to the unit hypersphere</li>
<li>and more…</li>
</ul>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>You can install the development version of embedplyr from <a href="https://github.com/" class="external-link">GitHub</a> with:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">remotes</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"rimonim/embedplyr"</span><span class="op">)</span> </span></code></pre></div>
</div>
<div class="section level2">
<h2 id="functionality">Functionality<a class="anchor" aria-label="anchor" href="#functionality"></a>
</h2>
<p>embedplyr is designed to facilitate the use of word and text embeddings in common data manipulation and text analysis workflows, without introducing new syntax or unfamiliar data structures.</p>
<p>embedplyr is model agnostic; it can be used to work with embeddings from decontextualized models like <a href="https://nlp.stanford.edu/projects/glove/" class="external-link">GloVe</a> and <a href="https://code.google.com/archive/p/word2vec/" class="external-link">word2vec</a>, or from contextualized models like BERT or others made available through the ‘<a href="https://r-text.org" class="external-link">text</a>’ package.</p>
<div class="section level3">
<h3 id="loading-pretrained-embeddings">Loading Pretrained Embeddings<a class="anchor" aria-label="anchor" href="#loading-pretrained-embeddings"></a>
</h3>
<p>embedplyr won’t help you train new embedding models, but it can load embeddings from a file or download them from online. This is especially useful for pretrained word embedding models like GloVe, word2vec, and fastText. Dozens of these models can be conveniently downloaded from online sources with <code><a href="reference/load_embeddings.html">load_embeddings()</a></code>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://rimonim.github.io/embedplyr/">embedplyr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">glove_twitter_25d</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/load_embeddings.html">load_embeddings</a></span><span class="op">(</span><span class="st">"glove.twitter.27B.25d"</span><span class="op">)</span></span></code></pre></div>
<p>The outcome is an embeddings object. An embeddings object is just a numeric matrix with fast hash table indexing by rownames (generally tokens). This means that it can be easily coerced to a dataframe or tibble, while also allowing special embeddings-specific methods and functions, such as <code><a href="reference/predict.embeddings.html">predict.embeddings()</a></code> and <code><a href="reference/find_nearest.html">find_nearest()</a></code>:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">moral_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">glove_twitter_25d</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"good"</span>, <span class="st">"bad"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">moral_embeddings</span></span>
<span><span class="co">#&gt; # 25-dimensional embeddings with 2 rows</span></span>
<span><span class="co">#&gt;      dim_1 dim_2 dim_3 dim_4 dim_5 dim_6 dim_7 dim_8 dim_9 dim..      </span></span>
<span><span class="co">#&gt; good -0.54  0.60 -0.15 -0.02 -0.14  0.60  2.19  0.21 -0.52 -0.23 ...  </span></span>
<span><span class="co">#&gt; bad   0.41  0.02  0.06 -0.01  0.27  0.71  1.64 -0.11 -0.26  0.11 ...</span></span>
<span></span>
<span><span class="fu"><a href="reference/find_nearest.html">find_nearest</a></span><span class="op">(</span><span class="va">glove_twitter_25d</span>, <span class="st">"dog"</span>, <span class="fl">5L</span>, method <span class="op">=</span> <span class="st">"cosine"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # 25-dimensional embeddings with 5 rows</span></span>
<span><span class="co">#&gt;        dim_1 dim_2 dim_3 dim_4 dim_5 dim_6 dim_7 dim_8 dim_9 dim..      </span></span>
<span><span class="co">#&gt; dog    -1.24 -0.36  0.57  0.37  0.60 -0.19  1.27 -0.37  0.09  0.40 ...  </span></span>
<span><span class="co">#&gt; cat    -0.96 -0.61  0.67  0.35  0.41 -0.21  1.38  0.13  0.32  0.66 ...  </span></span>
<span><span class="co">#&gt; dogs   -0.63 -0.11  0.22  0.27  0.28  0.13  1.44 -1.18 -0.26  0.60 ...  </span></span>
<span><span class="co">#&gt; horse  -0.76 -0.63  0.43  0.04  0.25 -0.18  1.08 -0.94  0.30  0.07 ...  </span></span>
<span><span class="co">#&gt; monkey -0.96 -0.38  0.49  0.66  0.21 -0.09  1.28 -0.11  0.27  0.42 ...</span></span></code></pre></div>
<p>Whereas indexing a regular matrix by rownames gets slower as the number of rows increases, embedingplyr’s hash table indexing means that token embeddings can be retrieved in milliseconds even from models with millions of rows.</p>
</div>
<div class="section level3">
<h3 id="similarity-metrics">Similarity Metrics<a class="anchor" aria-label="anchor" href="#similarity-metrics"></a>
</h3>
<p>Functions for similarity and distance metrics are as simple as possible; each one takes in vectors and outputs a scalar.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">vec1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">vec2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">vec3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">2</span>, <span class="op">-</span><span class="fl">13</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="reference/sim_metrics.html">dot_prod</a></span><span class="op">(</span><span class="va">vec1</span>, <span class="va">vec2</span><span class="op">)</span>                        <span class="co"># dot product</span></span>
<span><span class="co">#&gt; [1] 18</span></span>
<span><span class="fu"><a href="reference/sim_metrics.html">cos_sim</a></span><span class="op">(</span><span class="va">vec1</span>, <span class="va">vec2</span><span class="op">)</span>                         <span class="co"># cosine similarity</span></span>
<span><span class="co">#&gt; [1] 0.6708204</span></span>
<span><span class="fu"><a href="reference/sim_metrics.html">euc_dist</a></span><span class="op">(</span><span class="va">vec1</span>, <span class="va">vec2</span><span class="op">)</span>                        <span class="co"># Euclidean distance</span></span>
<span><span class="co">#&gt; [1] 4.242641</span></span>
<span><span class="fu"><a href="reference/sim_metrics.html">anchored_sim</a></span><span class="op">(</span><span class="va">vec1</span>, pos <span class="op">=</span> <span class="va">vec2</span>, neg <span class="op">=</span> <span class="va">vec3</span><span class="op">)</span>  <span class="co"># projection to an anchored vector</span></span>
<span><span class="co">#&gt; [1] 0.9887218</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="example-tidy-workflow">Example Tidy Workflow<a class="anchor" aria-label="anchor" href="#example-tidy-workflow"></a>
</h3>
<p>Given a tidy dataframe of texts, <code><a href="reference/textstat_embedding.html">embed_docs()</a></code> will generate embeddings by averaging the embeddings of words in each text (for more information on why this works well, see <a href="https://ds4psych.com/decontextualized-embeddings#sec-embedding-magnitude" class="external-link">Data Science for Psychology, Chapter 18</a>). By default, <code><a href="reference/textstat_embedding.html">embed_docs()</a></code> uses a simple unweighted mean, but other averaging methods are available.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span><span class="va">valence_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tribble.html" class="external-link">tribble</a></span><span class="op">(</span></span>
<span>    <span class="op">~</span><span class="va">id</span>,        <span class="op">~</span><span class="va">text</span>,</span>
<span>    <span class="st">"positive"</span>, <span class="st">"happy awesome cool nice"</span>,</span>
<span>    <span class="st">"neutral"</span>,  <span class="st">"ok fine sure whatever"</span>,</span>
<span>    <span class="st">"negative"</span>, <span class="st">"sad bad horrible angry"</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span><span class="va">valence_embeddings_df</span> <span class="op">&lt;-</span> <span class="va">valence_df</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="reference/textstat_embedding.html">embed_docs</a></span><span class="op">(</span><span class="st">"text"</span>, <span class="va">glove_twitter_25d</span>, id_col <span class="op">=</span> <span class="st">"id"</span>, .keep_all <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">valence_embeddings_df</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 27</span></span>
<span><span class="co">#&gt;   id       text       dim_1   dim_2    dim_3   dim_4   dim_5   dim_6 dim_7 dim_8</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 positive happy a… -0.584  -0.0810 -0.00361 -0.381   0.0786  0.646   1.66 0.543</span></span>
<span><span class="co">#&gt; 2 neutral  ok fine… -0.0293  0.169  -0.226   -0.175  -0.389  -0.0313  1.22 0.222</span></span>
<span><span class="co">#&gt; 3 negative sad bad…  0.296  -0.244   0.150    0.0809  0.155   0.728   1.51 0.122</span></span>
<span><span class="co">#&gt; # ℹ 17 more variables: dim_9 &lt;dbl&gt;, dim_10 &lt;dbl&gt;, dim_11 &lt;dbl&gt;, dim_12 &lt;dbl&gt;,</span></span>
<span><span class="co">#&gt; #   dim_13 &lt;dbl&gt;, dim_14 &lt;dbl&gt;, dim_15 &lt;dbl&gt;, dim_16 &lt;dbl&gt;, dim_17 &lt;dbl&gt;,</span></span>
<span><span class="co">#&gt; #   dim_18 &lt;dbl&gt;, dim_19 &lt;dbl&gt;, dim_20 &lt;dbl&gt;, dim_21 &lt;dbl&gt;, dim_22 &lt;dbl&gt;,</span></span>
<span><span class="co">#&gt; #   dim_23 &lt;dbl&gt;, dim_24 &lt;dbl&gt;, dim_25 &lt;dbl&gt;</span></span></code></pre></div>
<p><code><a href="reference/textstat_embedding.html">embed_docs()</a></code> can also be used to generate other types of embeddings. For example, we can use the ‘<a href="https://r-text.org" class="external-link">text</a>’ package to generate embeddings using any model available from Huggingface transformers.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># function that takes character vector and outputs a data frame</span></span>
<span><span class="va">sbert_embeddings</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">texts</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu">text</span><span class="fu">::</span><span class="fu"><a href="https://r-text.org/reference/textEmbed.html" class="external-link">textEmbed</a></span><span class="op">(</span></span>
<span>        <span class="va">texts</span>,</span>
<span>        model <span class="op">=</span> <span class="st">"sentence-transformers/all-MiniLM-L12-v2"</span>, <span class="co"># model name</span></span>
<span>        layers <span class="op">=</span> <span class="op">-</span><span class="fl">2</span>,  <span class="co"># second to last layer (default)</span></span>
<span>        tokens_select <span class="op">=</span> <span class="st">"[CLS]"</span>, <span class="co"># use only [CLS] token</span></span>
<span>        dim_name <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>        keep_token_embeddings <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>    <span class="op">)</span><span class="op">$</span><span class="va">texts</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">valence_sbert_df</span> <span class="op">&lt;-</span> <span class="va">valence_df</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="reference/textstat_embedding.html">embed_docs</a></span><span class="op">(</span><span class="st">"text"</span>, <span class="va">sbert_embeddings</span>, id_col <span class="op">=</span> <span class="st">"id"</span>, .keep_all <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>To quantify how good and how intense the texts are, we can compare them to the embeddings for “good” and “intense” using <code><a href="reference/get_sims.html">get_sims()</a></code>. Note that this step requires only a dataframe, tibble, or embeddings object with numeric columns; the embeddings can come from any source.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">good_vec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">glove_twitter_25d</span>, <span class="st">"good"</span><span class="op">)</span></span>
<span><span class="va">intense_vec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">glove_twitter_25d</span>, <span class="st">"intense"</span><span class="op">)</span></span>
<span><span class="va">valence_quantified</span> <span class="op">&lt;-</span> <span class="va">valence_embeddings_df</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="reference/get_sims.html">get_sims</a></span><span class="op">(</span></span>
<span>        <span class="va">dim_1</span><span class="op">:</span><span class="va">dim_25</span>, </span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>            good <span class="op">=</span> <span class="va">good_vec</span>, </span>
<span>            intense <span class="op">=</span> <span class="va">intense_vec</span></span>
<span>            <span class="op">)</span></span>
<span>        <span class="op">)</span></span>
<span><span class="va">valence_quantified</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 4</span></span>
<span><span class="co">#&gt;   id       text                     good intense</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 positive happy awesome cool nice 0.958   0.585</span></span>
<span><span class="co">#&gt; 2 neutral  ok fine sure whatever   0.909   0.535</span></span>
<span><span class="co">#&gt; 3 negative sad bad horrible angry  0.848   0.747</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="example-quanteda-workflow">Example Quanteda Workflow<a class="anchor" aria-label="anchor" href="#example-quanteda-workflow"></a>
</h3>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://quanteda.io" class="external-link">quanteda</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># corpus</span></span>
<span><span class="va">valence_corp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://quanteda.io/reference/corpus.html" class="external-link">corpus</a></span><span class="op">(</span><span class="va">valence_df</span>, docid_field <span class="op">=</span> <span class="st">"id"</span><span class="op">)</span></span>
<span><span class="va">valence_corp</span></span>
<span><span class="co">#&gt; Corpus consisting of 3 documents.</span></span>
<span><span class="co">#&gt; positive :</span></span>
<span><span class="co">#&gt; "happy awesome cool nice"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; neutral :</span></span>
<span><span class="co">#&gt; "ok fine sure whatever"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; negative :</span></span>
<span><span class="co">#&gt; "sad bad horrible angry"</span></span>
<span></span>
<span><span class="co"># dfm</span></span>
<span><span class="va">valence_dfm</span> <span class="op">&lt;-</span> <span class="va">valence_corp</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="https://quanteda.io/reference/tokens.html" class="external-link">tokens</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="https://quanteda.io/reference/dfm.html" class="external-link">dfm</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># compute embeddings</span></span>
<span><span class="va">valence_embeddings_df</span> <span class="op">&lt;-</span> <span class="va">valence_dfm</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="reference/textstat_embedding.html">textstat_embedding</a></span><span class="op">(</span><span class="va">glove_twitter_25d</span><span class="op">)</span></span>
<span><span class="va">valence_embeddings_df</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 26</span></span>
<span><span class="co">#&gt;   doc_id     dim_1   dim_2    dim_3   dim_4   dim_5   dim_6 dim_7 dim_8  dim_9</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 positive -0.584  -0.0810 -0.00361 -0.381   0.0786  0.646   1.66 0.543 -0.830</span></span>
<span><span class="co">#&gt; 2 neutral  -0.0293  0.169  -0.226   -0.175  -0.389  -0.0313  1.22 0.222 -0.394</span></span>
<span><span class="co">#&gt; 3 negative  0.296  -0.244   0.150    0.0809  0.155   0.728   1.51 0.122 -0.588</span></span>
<span><span class="co">#&gt; # ℹ 16 more variables: dim_10 &lt;dbl&gt;, dim_11 &lt;dbl&gt;, dim_12 &lt;dbl&gt;, dim_13 &lt;dbl&gt;,</span></span>
<span><span class="co">#&gt; #   dim_14 &lt;dbl&gt;, dim_15 &lt;dbl&gt;, dim_16 &lt;dbl&gt;, dim_17 &lt;dbl&gt;, dim_18 &lt;dbl&gt;,</span></span>
<span><span class="co">#&gt; #   dim_19 &lt;dbl&gt;, dim_20 &lt;dbl&gt;, dim_21 &lt;dbl&gt;, dim_22 &lt;dbl&gt;, dim_23 &lt;dbl&gt;,</span></span>
<span><span class="co">#&gt; #   dim_24 &lt;dbl&gt;, dim_25 &lt;dbl&gt;</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="other-functions">Other Functions<a class="anchor" aria-label="anchor" href="#other-functions"></a>
</h3>
<div class="section level4">
<h4 id="reduce-dimensionality">Reduce Dimensionality<a class="anchor" aria-label="anchor" href="#reduce-dimensionality"></a>
</h4>
<p>It is sometimes useful to reduce the dimensionality of embeddings. This is done with <code><a href="reference/reduce_dimensionality.html">reduce_dimensionality()</a></code>, which by default performs PCA without column normalization.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">valence_df_2d</span> <span class="op">&lt;-</span> <span class="va">valence_embeddings_df</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="reference/reduce_dimensionality.html">reduce_dimensionality</a></span><span class="op">(</span><span class="va">dim_1</span><span class="op">:</span><span class="va">dim_25</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">valence_df_2d</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 3</span></span>
<span><span class="co">#&gt;   doc_id      PC1    PC2</span></span>
<span><span class="co">#&gt; * &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 positive -1.47   0.494</span></span>
<span><span class="co">#&gt; 2 neutral   0.121 -1.13 </span></span>
<span><span class="co">#&gt; 3 negative  1.35   0.640</span></span></code></pre></div>
<p><code><a href="reference/reduce_dimensionality.html">reduce_dimensionality()</a></code> can also be used to apply the same rotation other embeddings not used to find the principle components.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">glove_twitter_25d</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"new"</span>, <span class="st">"strange"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># get rotation with `output_rotation = TRUE`</span></span>
<span><span class="va">valence_rotation_2d</span> <span class="op">&lt;-</span> <span class="va">valence_embeddings_df</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="reference/reduce_dimensionality.html">reduce_dimensionality</a></span><span class="op">(</span><span class="va">dim_1</span><span class="op">:</span><span class="va">dim_25</span>, <span class="fl">2</span>, output_rotation <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># apply the same rotation to new embeddings</span></span>
<span><span class="va">new_with_valence_rotation</span> <span class="op">&lt;-</span> <span class="va">new_embeddings</span> <span class="op">|&gt;</span> </span>
<span>    <span class="fu"><a href="reference/reduce_dimensionality.html">reduce_dimensionality</a></span><span class="op">(</span>custom_rotation <span class="op">=</span> <span class="va">valence_rotation_2d</span><span class="op">)</span></span>
<span><span class="va">new_with_valence_rotation</span></span>
<span><span class="co">#&gt; # 2-dimensional embeddings with 2 rows</span></span>
<span><span class="co">#&gt;         PC1   PC2  </span></span>
<span><span class="co">#&gt; new     -2.38  0.24</span></span>
<span><span class="co">#&gt; strange  0.09  1.18</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="normalize-scale-embeddings-to-the-unit-hypersphere">Normalize (Scale Embeddings to the Unit Hypersphere)<a class="anchor" aria-label="anchor" href="#normalize-scale-embeddings-to-the-unit-hypersphere"></a>
</h4>
<p><code><a href="reference/normalize.html">normalize()</a></code> and <code><a href="reference/normalize.html">normalize_rows()</a></code> scale embeddings such that their magnitude is 1, while their angle from the origin is unchanged.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/normalize.html">normalize</a></span><span class="op">(</span><span class="va">good_vec</span><span class="op">)</span></span>
<span><span class="co">#&gt;        dim_1        dim_2        dim_3        dim_4        dim_5        dim_6 </span></span>
<span><span class="co">#&gt; -0.090587846  0.100363800 -0.024215926 -0.003896062 -0.022930449  0.100135678 </span></span>
<span><span class="co">#&gt;        dim_7        dim_8        dim_9       dim_10       dim_11       dim_12 </span></span>
<span><span class="co">#&gt;  0.364995604  0.034641280 -0.085813930 -0.038466074 -0.133854478  0.094747331 </span></span>
<span><span class="co">#&gt;       dim_13       dim_14       dim_15       dim_16       dim_17       dim_18 </span></span>
<span><span class="co">#&gt; -0.836459360  0.044137493  0.079744546 -0.099664447  0.093466849 -0.181581983 </span></span>
<span><span class="co">#&gt;       dim_19       dim_20       dim_21       dim_22       dim_23       dim_24 </span></span>
<span><span class="co">#&gt; -0.087563977  0.020824065 -0.037671809  0.040843874 -0.076207818  0.154222299 </span></span>
<span><span class="co">#&gt;       dim_25 </span></span>
<span><span class="co">#&gt;  0.003684091</span></span>
<span></span>
<span><span class="fu"><a href="reference/normalize.html">normalize</a></span><span class="op">(</span><span class="va">moral_embeddings</span><span class="op">)</span></span>
<span><span class="co">#&gt; # 25-dimensional embeddings with 2 rows</span></span>
<span><span class="co">#&gt;      dim_1 dim_2 dim_3 dim_4 dim_5 dim_6 dim_7 dim_8 dim_9 dim..      </span></span>
<span><span class="co">#&gt; good -0.09  0.10 -0.02 -0.00 -0.02  0.10  0.36  0.03 -0.09 -0.04 ...  </span></span>
<span><span class="co">#&gt; bad   0.08  0.00  0.01 -0.00  0.05  0.13  0.31 -0.02 -0.05  0.02 ...</span></span>
<span></span>
<span><span class="va">valence_embeddings_df</span> <span class="op">|&gt;</span> <span class="fu"><a href="reference/normalize.html">normalize_rows</a></span><span class="op">(</span><span class="va">dim_1</span><span class="op">:</span><span class="va">dim_25</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 26</span></span>
<span><span class="co">#&gt;   doc_id    dim_1   dim_2    dim_3   dim_4   dim_5    dim_6 dim_7  dim_8   dim_9</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 posit… -0.118   -0.0163 -7.26e-4 -0.0767  0.0158  0.130   0.334 0.109  -0.167 </span></span>
<span><span class="co">#&gt; 2 neutr… -0.00633  0.0365 -4.87e-2 -0.0377 -0.0839 -0.00675 0.262 0.0479 -0.0850</span></span>
<span><span class="co">#&gt; 3 negat…  0.0666  -0.0549  3.38e-2  0.0182  0.0347  0.164   0.339 0.0274 -0.132 </span></span>
<span><span class="co">#&gt; # ℹ 16 more variables: dim_10 &lt;dbl&gt;, dim_11 &lt;dbl&gt;, dim_12 &lt;dbl&gt;, dim_13 &lt;dbl&gt;,</span></span>
<span><span class="co">#&gt; #   dim_14 &lt;dbl&gt;, dim_15 &lt;dbl&gt;, dim_16 &lt;dbl&gt;, dim_17 &lt;dbl&gt;, dim_18 &lt;dbl&gt;,</span></span>
<span><span class="co">#&gt; #   dim_19 &lt;dbl&gt;, dim_20 &lt;dbl&gt;, dim_21 &lt;dbl&gt;, dim_22 &lt;dbl&gt;, dim_23 &lt;dbl&gt;,</span></span>
<span><span class="co">#&gt; #   dim_24 &lt;dbl&gt;, dim_25 &lt;dbl&gt;</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="magnitude">Magnitude<a class="anchor" aria-label="anchor" href="#magnitude"></a>
</h4>
<p>The magnitude, norm, or length of a vector is its Euclidean distance from the origin.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/magnitude.html">magnitude</a></span><span class="op">(</span><span class="va">good_vec</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 6.005552</span></span>
<span></span>
<span><span class="fu"><a href="reference/magnitude.html">magnitude</a></span><span class="op">(</span><span class="va">moral_embeddings</span><span class="op">)</span></span>
<span><span class="co">#&gt;     good      bad </span></span>
<span><span class="co">#&gt; 6.005552 5.355951</span></span></code></pre></div>
</div>
</div>
</div>
</div>
  </main><aside class="col-md-3"><div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3)</small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing embedplyr</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Louis Teitelbaum <br><small class="roles"> Maintainer, author, copyright holder </small> <a href="https://orcid.org/0009-0001-9347-0145" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/rimonim/embedplyr/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/rimonim/embedplyr/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://codecov.io/github/rimonim/embedplyr" class="external-link"><img src="https://codecov.io/github/rimonim/embedplyr/graph/badge.svg?token=CNJZ8R2LYZ" alt="codecov test coverage"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Louis Teitelbaum.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
