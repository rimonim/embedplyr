---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
options(embeddings.model.path = "~/Documents/data/embeddings")
```

# embedplyr <img src="man/figures/logo.png" align="right" height="138" alt="" />

<!-- badges: start -->
[![R-CMD-check](https://github.com/rimonim/embedplyr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rimonim/embedplyr/actions/workflows/R-CMD-check.yaml)
[![codecov test coverage](https://codecov.io/github/rimonim/embedplyr/graph/badge.svg?token=CNJZ8R2LYZ)](https://codecov.io/github/rimonim/embedplyr)
<!-- badges: end -->

## Overview

embedplyr enables common operations with word and text embeddings within a 'tidyverse' and/or 'quanteda' workflow, as demonstrated in [Data Science for Psychology: Natural Language](http://ds4psych.com). 

* `load_embeddings()` loads pretrained [GloVe](https://nlp.stanford.edu/projects/glove/), [word2vec](https://code.google.com/archive/p/word2vec/), [HistWords](https://nlp.stanford.edu/projects/histwords/), [ConceptNet Numberbatch](https://github.com/commonsense/conceptnet-numberbatch), and [fastText](https://fasttext.cc) word embedding models from Internet sources or from your working directory
* `embed_tokens()` returns the embedding for each token in a set of texts
* `embed_docs()` generates text embeddings for a set of documents
* `get_sims()` calculates row-wise similarity metrics between a set of embeddings and a given reference
* `average_embedding()` calculates the (weighted) average of multiple embeddings
* `reduce_dimensionality()` reduces the dimensionality of embeddings
* `align_embeddings()` rotates the embeddings from one model so that they can be compared with those from another
* `normalize()` and `normalize_rows()` normalize embeddings to the unit hypersphere
* and more...

## Installation

You can install the development version of embedplyr from [GitHub](https://github.com/) with:

``` r
remotes::install_github("rimonim/embedplyr") 
```

## Functionality

embedplyr is designed to facilitate the use of word and text embeddings in common data manipulation and text analysis workflows, without introducing new syntax or unfamiliar data structures.

embedplyr is model agnostic; it can be used to work with embeddings from decontextualized models like [GloVe](https://nlp.stanford.edu/projects/glove/) and [word2vec](https://code.google.com/archive/p/word2vec/), or from contextualized models like BERT or others made available through the '[text](https://r-text.org)' package.

### Loading Pretrained Embeddings

embedplyr won't help you train new embedding models, but it can load embeddings from a file or download them from online. This is especially useful for pretrained word embedding models like GloVe, word2vec, and fastText. Dozens of these models can be conveniently downloaded from online sources with `load_embeddings()`.

```{r, warning=FALSE, message=FALSE}
library(embedplyr)

glove_twitter_25d <- load_embeddings("glove.twitter.27B.25d")
```

The outcome is an embeddings object. An embeddings object is just a numeric matrix with fast hash table indexing by rownames (generally tokens). This means that it can be easily coerced to a dataframe or tibble, while also allowing special embeddings-specific methods and functions, such as `predict.embeddings()` and `find_nearest()`:

```{r}
moral_embeddings <- predict(glove_twitter_25d, c("good", "bad"))
moral_embeddings

find_nearest(glove_twitter_25d, "dog", 5L, method = "cosine")
```

Whereas indexing a regular matrix by rownames gets slower as the number of rows increases, embedingplyr's hash table indexing means that token embeddings can be retrieved in milliseconds even from models with millions of rows.

### Similarity Metrics

Functions for similarity and distance metrics are as simple as possible; each one takes in vectors and outputs a scalar.

```{r}
vec1 <- c(1, 5, 2)
vec2 <- c(4, 2, 2)
vec3 <- c(-1, -2, -13)

dot_prod(vec1, vec2)            		    # dot product
cos_sim(vec1, vec2)             		    # cosine similarity
euc_dist(vec1, vec2)            		    # Euclidean distance
anchored_sim(vec1, pos = vec2, neg = vec3)  # projection to an anchored vector
```

### Example Tidy Workflow

Given a tidy dataframe of texts, `embed_docs()` will generate embeddings by averaging the embeddings of words in each text (for more information on why this works well, see [Data Science for Psychology, Chapter 18](https://ds4psych.com/decontextualized-embeddings#sec-embedding-magnitude)). By default, `embed_docs()` uses a simple unweighted mean, but other averaging methods are available.

```{r, warning=FALSE, message=FALSE}
library(dplyr)
valence_df <- tribble(
	~id,        ~text,
	"positive", "happy awesome cool nice",
	"neutral",  "ok fine sure whatever",
	"negative", "sad bad horrible angry"
	)

valence_embeddings_df <- valence_df |> 
	embed_docs("text", glove_twitter_25d, id_col = "id", .keep_all = TRUE)
valence_embeddings_df
```

`embed_docs()` can also be used to generate other types of embeddings. For example, we can use the '[text](https://r-text.org)' package to generate embeddings using any model available from Huggingface transformers.

```{r, eval=FALSE}
# function that takes character vector and outputs a data frame
sbert_embeddings <- function(texts) {
	text::textEmbed(
		texts,
		model = "sentence-transformers/all-MiniLM-L12-v2", # model name
		layers = -2,  # second to last layer (default)
		tokens_select = "[CLS]", # use only [CLS] token
		dim_name = FALSE,
		keep_token_embeddings = FALSE
	)$texts[[1]]
}

valence_sbert_df <- valence_df |> 
	embed_docs("text", sbert_embeddings, id_col = "id", .keep_all = TRUE)
```

To quantify how good and how intense the texts are, we can compare them to the embeddings for "good" and "intense" using `get_sims()`. Note that this step requires only a dataframe,  tibble, or embeddings object with numeric columns; the embeddings can come from any source. 

```{r}
good_vec <- predict(glove_twitter_25d, "good")
intense_vec <- predict(glove_twitter_25d, "intense")
valence_quantified <- valence_embeddings_df |> 
	get_sims(
		dim_1:dim_25, 
		list(
			good = good_vec, 
			intense = intense_vec
			)
		)
valence_quantified
```
### Example Quanteda Workflow

```{r, warning=FALSE, message=FALSE}
library(quanteda)

# corpus
valence_corp <- corpus(valence_df, docid_field = "id")
valence_corp

# dfm
valence_dfm <- valence_corp |> 
	tokens() |> 
	dfm()

# compute embeddings
valence_embeddings_df <- valence_dfm |> 
	textstat_embedding(glove_twitter_25d)
valence_embeddings_df
```

### Other Functions

#### Reduce Dimensionality

It is sometimes useful to reduce the dimensionality of embeddings. This is done with `reduce_dimensionality()`, which by default performs PCA without column normalization.

```{r}
valence_df_2d <- valence_embeddings_df |> 
	reduce_dimensionality(dim_1:dim_25, 2)
valence_df_2d
```
`reduce_dimensionality()` can also be used to apply the same rotation to other embeddings not used to find the principle components. 

```{r}
new_embeddings <- predict(glove_twitter_25d, c("new", "strange"))

# get rotation with `output_rotation = TRUE`
valence_rotation_2d <- valence_embeddings_df |> 
	reduce_dimensionality(dim_1:dim_25, 2, output_rotation = TRUE)

# apply the same rotation to new embeddings
new_with_valence_rotation <- new_embeddings |> 
	reduce_dimensionality(custom_rotation = valence_rotation_2d)
new_with_valence_rotation
```

#### Align and Compare Embeddings Models

Aligning separately trained embeddings can be useful for tracking semantic change over time or semantic differences between training datasets. It can also be useful for comparing texts across languages. `align_embeddings(x, y)` rotates the embeddings in `x` (and changes their dimensionality if necessary) so that they can be compared with those in `y`. Optionally, `matching` can be used to specify one-to-one matching between  embeddings in the two models (e.g. a bilingual dictionary).

Once aligned, groups of embeddings can be compared using `total_dist()` or `average_sim()`. These can be used to quantify the overall distance or similarity between two parallel embedding spaces.

#### Normalize (Scale Embeddings to the Unit Hypersphere)

`normalize()` and `normalize_rows()` scale embeddings such that their magnitude is 1, while their angle from the origin is unchanged.

```{r}
normalize(good_vec)

normalize(moral_embeddings)

valence_embeddings_df |> normalize_rows(dim_1:dim_25)
```

#### Magnitude

The magnitude, norm, or length of a vector is its Euclidean distance from the origin.

```{r}
magnitude(good_vec)

magnitude(moral_embeddings)
```
